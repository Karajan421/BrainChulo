{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823bb6e4-5f01-4ef2-a794-6e997cbfd8f1",
   "metadata": {},
   "source": [
    "## Define a tool\n",
    "We define a search tool with GoogleSerperAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b92049-03e4-466c-824a-f46de7490a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "os.environ[\"SERPER_API_KEY\"] = 'fbac5061b434c6b0e5f55968258b144209993ab2'\n",
    "search = GoogleSerperAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "llama = guidance.llms.LlamaCpp(\n",
    "    model =\"/home/karajan/Downloads/Manticore-13B.ggmlv3.q4_0.bin\",\n",
    "    tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n",
    "    before_role = \"<|\",\n",
    "    after_role = \"|>\",\n",
    "    n_gpu_layers=300,\n",
    "    n_threads=12,\n",
    "    caching=False, )\n",
    "guidance.llm = llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "model_type = \"LlamaCpp\"\n",
    "model_path = \"/home/karajan/Downloads/Manticore-13B.ggmlv3.q4_0.bin\"\n",
    "model_n_ctx =1000\n",
    "target_source_chunks = 4\n",
    "n_gpu_layers = 500\n",
    "use_mlock = 0\n",
    "n_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\n",
    "callbacks = []\n",
    "qa_prompt = \"\"\n",
    "llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "import re \n",
    "from colorama import Fore, Style\n",
    "\n",
    "retriver=\"\"\n",
    "EMBEDDINGS_MAP = {\n",
    "    **{name: HuggingFaceInstructEmbeddings for name in [\"hkunlp/instructor-xl\", \"hkunlp/instructor-large\"]},\n",
    "    **{name: HuggingFaceEmbeddings for name in [\"all-MiniLM-L6-v2\", \"sentence-t5-xxl\"]}\n",
    "}\n",
    "EMBEDDINGS_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove line breaks\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_unstructured_document(document: str) -> list[Document]:\n",
    "    with open(document, 'r') as file:\n",
    "        text = file.read()\n",
    "    title = os.path.basename(document)\n",
    "    return [Document(page_content=text, metadata={\"title\": title})]\n",
    "\n",
    "def split_documents(documents: list[Document], chunk_size: int = 250, chunk_overlap: int = 20) -> list[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "def ingest_file(file_path):\n",
    "        # Load unstructured document\n",
    "        documents = load_unstructured_document(file_path)\n",
    "\n",
    "        # Split documents into chunks\n",
    "        documents = split_documents(documents, chunk_size=250, chunk_overlap=100)\n",
    "\n",
    "        # Determine the embedding model to use\n",
    "        EmbeddingsModel = EMBEDDINGS_MAP.get(EMBEDDINGS_MODEL)\n",
    "        if EmbeddingsModel is None:\n",
    "            raise ValueError(f\"Invalid embeddings model: {EMBEDDINGS_MODEL}\")\n",
    "\n",
    "        model_kwargs = {\"device\": \"cuda:0\"} if EmbeddingsModel == HuggingFaceInstructEmbeddings else {}\n",
    "        embedding = EmbeddingsModel(model_name=EMBEDDINGS_MODEL, model_kwargs=model_kwargs)\n",
    "\n",
    "        # Store embeddings from the chunked documents\n",
    "        vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n",
    "\n",
    "        retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "\n",
    "        print(file_path)\n",
    "        print(retriever)\n",
    "\n",
    "        return retriever, file_path\n",
    "\n",
    "\n",
    "def checkQuestion(question: str, retriever, llm):\n",
    "    global qa_prompt\n",
    "    question = question.replace(\"Action Input: \", \"\")\n",
    "    QUESTION_CHECK_PROMPT_TEMPLATE = \"\"\"###Instruction: You are an AI assistant who uses document information to answer questions. You MUST answer with 'yes' or 'no'. Given the following pieces of context, determine if there are any elements related to the question in the context. To assist me in this task, you have access to a vector database context that contains various documents related to different topics.\n",
    "Context:{context}\n",
    "Don't forget you MUST answer with 'yes' or 'no'\n",
    "Question: Is there any info related to \"\"{question}\"\" in the context? Yes or No?\n",
    "### Response:\n",
    "\"\"\"\n",
    "    QUESTION_QA_PROMPT_TEMPLATE = \"\"\"### Human: You are an helpful assistant that tries to answer questions concisely and precisely. Your answer must ONLY be based on the context information provided.\n",
    "    Context:{context}\n",
    "    Question: {question}\n",
    "    ### Assistant:\n",
    "\"\"\"\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "    # Answer the question\n",
    "    answer_data = qa({\"query\": question})\n",
    "\n",
    "    # Check if 'answer' is in answer_data, if not print it in bold red\n",
    "    if 'result' not in answer_data:\n",
    "        print(f\"\\033[1;31m{answer_data}\\033[0m\")\n",
    "        return \"Issue in retrieving the answer.\"\n",
    "\n",
    "    answer = answer_data['result']\n",
    "    context_documents = answer_data['source_documents']\n",
    "\n",
    "    # Combine all contexts into one\n",
    "    context = \" \".join([clean_text(doc.page_content) for doc in context_documents])\n",
    "    # Formulate the prompt for the LLM\n",
    "    question_check_prompt = QUESTION_CHECK_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    qa_prompt = QUESTION_QA_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "    print(Fore.GREEN + Style.BRIGHT + question_check_prompt + Style.RESET_ALL)\n",
    "    # Submit the prompt to the LLM directly\n",
    "    # answerable = llm(question_check_prompt)\n",
    "    # print(Fore.GREEN + Style.BRIGHT + answerable + Style.RESET_ALL)\n",
    "    # if \"yes\" in answerable.lower():\n",
    "    #    return answerable\n",
    "    # else:\n",
    "    #     return answerable\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever, file_patch = ingest_file(\"/home/karajan/Documents/notion.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de04fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkQuestion(\"What's the wifi code\", retriever, llm))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "116eab4f-f7b5-4428-9827-7600bd4d4cdd",
   "metadata": {},
   "source": [
    "## Let's test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0812ee13-8252-4488-bf0f-51872d28c66e",
   "metadata": {},
   "source": [
    "# Build the agent with Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213706a-9b64-4d8b-b769-13b163367ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm.cache.clear()\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "041d69e8-78a8-499c-a34c-1c98eea2b23a",
   "metadata": {},
   "source": [
    "### Define prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea7e11-244b-4aad-8f54-688b5f827920",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Check Question', 'Chroma Search']\n",
    "\n",
    "prompt_start_template = \"\"\"{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "Check Question: A tool to validate if a question is answerable or not. The input is the question to validate.\n",
    "\n",
    "Chroma Search: A wrapper around Chroma Search. Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "\n",
    "Strictly use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [Check Question, Chroma Search]\n",
    "Action Input: the input to the action, should be a question.\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "For examples:\n",
    "Question: How old is CEO of Microsoft wife?\n",
    "Thought: First, I need to find who is the CEO of Microsoft.\n",
    "Action: Google Search\n",
    "Action Input: Who is the CEO of Microsoft?\n",
    "Observation: Satya Nadella is the CEO of Microsoft.\n",
    "Thought: Now, I should find out Satya Nadella's wife.\n",
    "Action: Google Search\n",
    "Action Input: Who is Satya Nadella's wife?\n",
    "Observation: Satya Nadella's wife's name is Anupama Nadella.\n",
    "Thought: Then, I need to check Anupama Nadella's age.\n",
    "Action: Google Search\n",
    "Action Input: How old is Anupama Nadella?\n",
    "Observation: Anupama Nadella's age is 50.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: Anupama Nadella is 50 years old.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "### Input:\n",
    "Question:{{question}}\n",
    "Start by checking your database to see if it contains relevant information to the question.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Question:{{question}}\n",
    "{{gen 'thought' stop='\\\\n'}}\n",
    "{{select 'answer' logprobs='logprobs' options=valid_answers}}:\n",
    "{{~/assistant}}\n",
    " \"\"\"\n",
    "\n",
    "prompt_mid_template = \"\"\"\n",
    "{{#assistant~}}\n",
    "{{history}}{{select 'tool_name' logprobs='logprobs' options=valid_tools}}\n",
    "{{gen 'actInput' stop='\\\\n'}}\n",
    "{{do_tool tool_name actInput}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Observation:{{gen 'thought' stop='\\\\n'}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{select 'answer' logprobs='logprobs' options=valid_answers}}: \n",
    "{{~/assistant}}\n",
    "\"\"\"\n",
    "\n",
    "prompt_final_template = \"\"\"\n",
    "{{#assistant~}}\n",
    "{{history}}{{select 'tool_name' options=valid_tools}}\n",
    "{{gen 'actInput' stop='\\\\n'}}\n",
    "{{do_tool tool_name actInput}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'thought' stop='\\\\n'}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{select 'answer' logprobs='logprobs' options=valid_answers}}: {{gen 'fn' stop='\\\\n'}}\n",
    "{{~/assistant}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46760ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the code of the wifi?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00e6ad72-f7a4-444f-a816-712aae88d65f",
   "metadata": {},
   "source": [
    "### Define tools, feel free to add more tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab90dcf-2c48-4073-bcef-54c7f5f36d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchGoogle(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "def qaChroma(key_word):\n",
    "        print(Fore.GREEN + Style.BRIGHT + qa_prompt + Style.RESET_ALL)\n",
    "        qa = llm(qa_prompt)\n",
    "        # qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n",
    "\n",
    "        print(qa)\n",
    "        #res = qa.run(key_word)\n",
    "        #print(res)\n",
    "        #return res\n",
    "        return qa\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': searchGoogle,\n",
    "    'Chroma Search': qaChroma\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "564b8576-0d21-4495-93c7-a3a67451782c",
   "metadata": {},
   "source": [
    "### Our agent with Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f9585-b42a-455f-a504-1b16ca6ed444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgentGuidance:\n",
    "    def __init__(self, guidance, tools, num_iter=3):\n",
    "        self.guidance = guidance\n",
    "        self.tools = tools\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def do_tool(self, tool_name, actInput):\n",
    "        return self.tools[tool_name](actInput)\n",
    "    \n",
    "    def __call__(self, query):\n",
    "        prompt_start = self.guidance(prompt_start_template)\n",
    "        result_start = prompt_start(question=query, valid_answers=valid_answers)\n",
    "\n",
    "        result_mid = result_start\n",
    "        \n",
    "        for _ in range(self.num_iter - 1):\n",
    "            if result_mid['answer'] == 'Final Answer':\n",
    "                break\n",
    "            history = result_mid.__str__()\n",
    "            prompt_mid = self.guidance(prompt_mid_template)\n",
    "            result_mid = prompt_mid(history=history, do_tool=self.do_tool, valid_answers=valid_answers, valid_tools=valid_tools)\n",
    "        \n",
    "        if result_mid['answer'] != 'Final Answer':\n",
    "            history = result_mid.__str__()\n",
    "            prompt_mid = self.guidance(prompt_final_template)\n",
    "            result_final = prompt_mid(history=history, do_tool=self.do_tool, valid_answers=['Final Answer'], valid_tools=valid_tools)\n",
    "        else:\n",
    "            history = result_mid.__str__()\n",
    "            prompt_mid = self.guidance(history + \"{{gen 'fn' stop='\\\\n'}}\")\n",
    "            result_final = prompt_mid()\n",
    "        return result_final['fn']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf966ea-1398-4bd0-821b-a62142bbf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_agent = CustomAgentGuidance(guidance, dict_tools)\n",
    "\n",
    "final_answer = custom_agent(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163881c-23a1-4a09-8e4d-34d849fb1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm.cache.clear()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93614b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_agent = CustomAgentGuidance(guidance, dict_tools)\n",
    "\n",
    "final_answer = custom_agent(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
